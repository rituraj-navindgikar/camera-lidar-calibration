{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0895cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.extract_topics import RosbagSensorExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9e924",
   "metadata": {},
   "source": [
    "The Problem in Simple Terms\n",
    "Think of it like this:\n",
    "\n",
    "Your camera takes a photo at 3:00:00 PM\n",
    "Your LiDAR scans at 3:00:00.05 PM (50 milliseconds later)\n",
    "During those 50ms, your car/robot moved a little bit\n",
    "If you just overlay them without accounting for movement, they won't match up\n",
    "\n",
    "The IMU tells you: \"Hey, during those 50ms, you rotated 2 degrees left and moved 10cm forward\"\n",
    "What is Preintegration?\n",
    "Preintegration is just adding up all the tiny movements the IMU measured.\n",
    "Think of it like this:\n",
    "\n",
    "IMU runs at 100 Hz (100 measurements per second)\n",
    "Between your camera (t=3.00) and LiDAR (t=3.05), you have maybe 5 IMU measurements\n",
    "Each one says: \"you rotated a tiny bit\" and \"you accelerated a tiny bit\"\n",
    "You integrate (add them up) to get the total change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934dda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUPreintegration:\n",
    "    def __init__(self):\n",
    "        self.delts_p = np.zeros(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b76e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading bag: dataset/snell_floor_2_ros2_bag\n",
      "[INFO] Camera topic: /camera_array/cam0/image_raw\n",
      "[INFO] LiDAR topic:  /ouster/points\n",
      "[INFO] IMU topic:    /imu/imu_compensated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 7327/7327 [00:22<00:00, 330.42msg/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Extraction complete:\n",
      "  Camera frames: 637\n",
      "  LiDAR scans:   318\n",
      "  IMU samples:   6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extractor = RosbagSensorExtractor(\n",
    "        bag_path=\"dataset/snell_floor_2_ros2_bag\",\n",
    "        camera_topic=\"/camera_array/cam0/image_raw\",\n",
    "        lidar_topic=\"/ouster/points\",\n",
    "        imu_topic=\"/imu/imu_compensated\",\n",
    "    )\n",
    "    \n",
    "    extractor.extract()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c958885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CAM]\n",
      "t_sec: 1679007787.4866676\n",
      "frame_id: cam_0_optical_frame\n",
      "image shape: (540, 720, 3)\n",
      "\n",
      "[LIDAR]\n",
      "t_sec: 1679007787.5265582\n",
      "frame_id: os_sensor\n",
      "points shape: (524288, 3)\n",
      "\n",
      "[IMU]\n",
      "t_sec: 1679007787.4450188\n",
      "frame_id: imu\n",
      "acc: [ -4.6586423   2.3057532 -12.003281 ]\n",
      "gyro: [-0.00290214 -0.00849772 -0.08366963]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[CAM]\")\n",
    "print(f\"t_sec: {extractor.camera_frames[0]['timestamp_sec']}\")\n",
    "print(f\"frame_id: {extractor.camera_frames[0]['frame_id']}\")\n",
    "print(f\"image shape: {extractor.camera_frames[0]['image'].shape}\")\n",
    "\n",
    "print(f\"\\n[LIDAR]\")\n",
    "print(f\"t_sec: {extractor.lidar_scans[0]['timestamp_sec']}\")\n",
    "print(f\"frame_id: {extractor.lidar_scans[0]['frame_id']}\")\n",
    "print(f\"points shape: {extractor.lidar_scans[0]['points'].shape}\")\n",
    "\n",
    "print(f\"\\n[IMU]\")\n",
    "print(f\"t_sec: {extractor.imu_measurements[0]['timestamp_sec']}\")\n",
    "print(f\"frame_id: {extractor.imu_measurements[0]['frame_id']}\")\n",
    "print(f\"acc: {extractor.imu_measurements[0]['linear_acceleration']}\")\n",
    "print(f\"gyro: {extractor.imu_measurements[0]['angular_velocity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfcbc5",
   "metadata": {},
   "source": [
    "three things to compute\n",
    "\n",
    "ΔR (Delta Rotation) - How much did you rotate?\n",
    "Δv (Delta Velocity) - How much did your speed change?\n",
    "Δp (Delta Position) - How much did you move?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b5a40",
   "metadata": {},
   "source": [
    "Step 1: Get Your Synchronized Data\n",
    "First, find which camera, LiDAR, and IMU data go together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde83eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have camera at time t\n",
    "camera_frame = extractor.camera_frames[0]\n",
    "t_camera = camera_frame['timestamp_sec']\n",
    "\n",
    "# Find closest LiDAR scan (this will be at t + delta_t)\n",
    "# Which LiDAR scan is closest to this camera time\n",
    "lidar_scan = # find closest one\n",
    "\n",
    "t_lidar = lidar_scan['timestamp_sec']\n",
    "delta_t = t_lidar - t_camera  # This is your time gap\n",
    "\n",
    "# Get ALL IMU measurements between camera and LiDAR\n",
    "imu_between = # all IMU data where t_camera <= t <= t_lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efca624",
   "metadata": {},
   "source": [
    "Step 2: Initialize Your Preintegration\n",
    "Before you start adding up movements, start from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with no change\n",
    "delta_rotation = np.eye(3)  # Identity matrix (no rotation)\n",
    "delta_velocity = np.zeros(3)  # Not moving\n",
    "delta_position = np.zeros(3)  # Haven't moved\n",
    "\n",
    "\n",
    "### Step 3: Loop Through Each IMU Measurement\n",
    "\n",
    "# you have 5 IMU measurements between camera and LiDAR:\n",
    "\n",
    "# Camera ----[IMU1]----[IMU2]----[IMU3]----[IMU4]----[IMU5]---- LiDAR\n",
    "#   t=3.00                                                    t=3.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each measurement we do\n",
    "for i in range(len(imu_between)):\n",
    "    current_imu = imu_between[i]\n",
    "    \n",
    "    # Get the measurements\n",
    "    gyro = current_imu['angular_velocity']  # tells  rotation rate\n",
    "    acc = current_imu['linear_acceleration']  # tells acceleration\n",
    "    \n",
    "    # How much time passed since last measurement\n",
    "    if i == 0:\n",
    "        dt = current_imu['timestamp_sec'] - t_camera\n",
    "    else:\n",
    "        dt = current_imu['timestamp_sec'] - imu_between[i-1]['timestamp_sec']\n",
    "    \n",
    "    # Now update your deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca2c05",
   "metadata": {},
   "source": [
    "Step 4: The Integration Math (Simplified)\n",
    "For each IMU measurement, you update three things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Update Rotation:\n",
    "\n",
    "# Gyroscope tells you: \"rotating at X rad/s\"\n",
    "# Over dt seconds, that's: X * dt total rotation\n",
    "angle_change = gyro * dt  # This is a 3D vector [roll, pitch, yaw] change\n",
    "\n",
    "# Convert this small rotation to a rotation matrix and multiply\n",
    "delta_rotation = delta_rotation @ rotation_from_vector(angle_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Update Velocity:\n",
    "\n",
    "# Acceleration needs to be in GLOBAL frame, not sensor frame\n",
    "# That's why we rotate it by current delta_rotation\n",
    "acc_global = delta_rotation @ acc\n",
    "\n",
    "# Remove gravity (gravity is always pulling down at 9.81 m/s²)\n",
    "acc_global -= np.array([0, 0, 9.81])\n",
    "\n",
    "# Velocity = old velocity + acceleration * time\n",
    "delta_velocity += acc_global * dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Update Position:\n",
    "\n",
    "# Position = old position + velocity * time + 0.5 * acceleration * time²\n",
    "delta_position += delta_velocity * dt + 0.5 * (delta_rotation @ acc) * dt**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93029d24",
   "metadata": {},
   "source": [
    "Step 5: Final Result\n",
    "After looping through all IMU measurements, you have:\n",
    "\n",
    "delta_rotation - a 3x3 rotation matrix\n",
    "delta_velocity - a 3D vector [vx, vy, vz]\n",
    "delta_position - a 3D vector [x, y, z]\n",
    "\n",
    "This tells you: \"Between camera time and LiDAR time, the sensor moved THIS much\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a628c35",
   "metadata": {},
   "source": [
    "Why Does This Help?\n",
    "Now you can transform the LiDAR points backwards in time to match the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiDAR point at time t+Δt\n",
    "lidar_point = np.array([x, y, z])\n",
    "\n",
    "# Move it back to camera time t\n",
    "lidar_point_at_camera_time = delta_rotation.T @ (lidar_point - delta_position)\n",
    "\n",
    "# Now it aligns with the camera image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pick a camera frame at time t_cam\n",
    "camera_frame = extractor.camera_frames[i]\n",
    "t_cam = camera_frame['timestamp_sec']\n",
    "\n",
    "# 2. Find closest LiDAR scan at time t_lidar\n",
    "t_lidar = find_closest_lidar(t_cam)\n",
    "\n",
    "# 3. Get ALL IMU between these times\n",
    "imu_measurements = get_imu_between(t_cam, t_lidar)\n",
    "\n",
    "# 4. Integrate ALL of them to get Tv\n",
    "Tv = preintegrate_imu(imu_measurements)\n",
    "\n",
    "# 5. Later: use line features to correct errors in Tv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb8d2c",
   "metadata": {},
   "source": [
    "1. Why Rotate Acceleration? (You're asking the RIGHT question!)\n",
    "This is actually the trickiest part to understand. Let me explain with a concrete example:\n",
    "Imagine your IMU is tilted 45 degrees:\n",
    "        IMU Frame (tilted)\n",
    "            ↑ z\n",
    "           /\n",
    "          /\n",
    "         /___→ x\n",
    "\n",
    "\n",
    "    Global Frame (level with ground)\n",
    "         ↑ z\n",
    "         |\n",
    "         |___→ x\n",
    "Now, you accelerate forward (in global x-direction):\n",
    "\n",
    "Global frame sees: acceleration = [1, 0, 0] (moving in x)\n",
    "IMU sees: acceleration = [0.7, 0, 0.7] (because it's tilted, it feels acceleration in BOTH its x and z axes!)\n",
    "\n",
    "The key insight: The IMU always reports acceleration in its own tilted coordinate frame, not the global frame.\n",
    "As you integrate and the sensor rotates more, the IMU frame keeps changing. So you need to rotate each acceleration measurement into the global frame before adding them up.\n",
    "Another way to think about it:\n",
    "\n",
    "Step 1: IMU tilted 0° reads acc = [1, 0, 0] in its frame\n",
    "Step 2: IMU tilted 10° reads acc = [1, 0, 0] in its frame\n",
    "If you just add them: [1,0,0] + [1,0,0] = [2,0,0] ← WRONG! They were pointing slightly different directions!\n",
    "You need to rotate each to global frame first, THEN add them.\n",
    "\n",
    "Does this make more sense now?\n",
    "\n",
    "2. Gravity Subtraction (You're PARTIALLY right!)\n",
    "You're thinking like a real robotics engineer! Let me clarify:\n",
    "You're RIGHT that:\n",
    "\n",
    "Gravity varies by location (9.78 at equator, 9.83 at poles)\n",
    "IMU precision is high (often 0.001 m/s² resolution)\n",
    "Gravity can be used for alignment\n",
    "\n",
    "But here's why we subtract it for preintegration:\n",
    "The IMU accelerometer measures specific force, not true acceleration. When sitting still on a table:\n",
    "\n",
    "True acceleration = 0 (not moving)\n",
    "IMU reads = [0, 0, 9.81] (feels the table pushing up against gravity)\n",
    "\n",
    "When you're moving:\n",
    "\n",
    "IMU reads = true_acceleration + gravity_effect\n",
    "To get true_acceleration = IMU_reading - gravity\n",
    "\n",
    "Your concern about precision is valid but:\n",
    "\n",
    "For short-term integration (50ms), using 9.81 is fine\n",
    "The error from using 9.81 instead of 9.806 for 50ms is: 0.006 * 0.05 = 0.0003 m ← negligible!\n",
    "For longer integration (multiple seconds), yes, you'd want accurate gravity\n",
    "\n",
    "About using gravity for alignment:\n",
    "You're absolutely right! In a full SLAM system, you would:\n",
    "\n",
    "Use gravity direction to estimate initial IMU orientation (pitch/roll)\n",
    "THEN do preintegration with gravity removed\n",
    "\n",
    "For this calibration project, we'll assume the IMU is already roughly aligned.\n",
    "So the answer: Subtract gravity for preintegration, but you're right that gravity is useful for initial alignment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6761c5",
   "metadata": {},
   "source": [
    "we already have the TF between all?EXCELLENT CATCH! You're thinking ahead!\n",
    "Yes, you're absolutely right that we have extrinsic calibrations (transforms) between sensors. But here's the subtle distinction:\n",
    "Two Different Transforms\n",
    "1. Extrinsic Transform (Static):\n",
    "T_imu_to_camera (or T_imu_to_lidar)\n",
    "\n",
    "This is FIXED, never changes\n",
    "Tells you where IMU sits relative to camera/lidar on the robot\n",
    "Example: \"IMU is 10cm behind camera, rotated 5° down\"\n",
    "\n",
    "2. Motion Transform (Dynamic - what we're computing):\n",
    "T_motion = change from time t to time t+Δt\n",
    "\n",
    "This CHANGES every moment\n",
    "Tells you how the ENTIRE robot moved\n",
    "Example: \"Between t and t+Δt, the whole robot rotated 2° and moved 5cm forward\"\n",
    "\n",
    "Why We Still Need to Rotate Acceleration\n",
    "Look at the paper's equation (Section III.A):\n",
    "T = Te × Tv\n",
    "Where:\n",
    "\n",
    "Te = extrinsic calibration (static, what you're thinking of)\n",
    "Tv = vehicle motion estimate (dynamic, what IMU preintegration gives you)\n",
    "\n",
    "The IMU preintegration is computing Tv - how did the vehicle body (carrying ALL sensors) move?\n",
    "Even though we know Te (where IMU sits on the robot), we still need to:\n",
    "\n",
    "Read IMU accelerations in IMU frame\n",
    "Rotate them to vehicle body frame as we integrate\n",
    "This gives us Tv (how the body moved)\n",
    "\n",
    "Concrete Example\n",
    "t=0: Robot at position [0, 0, 0], facing north\n",
    "     IMU reads acc = [1, 0, 0] in its own frame\n",
    "\n",
    "t=0.01s: Robot rotated 10° clockwise\n",
    "         IMU reads acc = [1, 0, 0] in its own frame (still!)\n",
    "         \n",
    "t=0.02s: Robot rotated 20° clockwise\n",
    "         IMU reads acc = [1, 0, 0] in its own frame (still!)\n",
    "Even though the IMU always reports in its own frame, the robot is rotating underneath it. So you need to account for that rotation when integrating.\n",
    "The extrinsic Te doesn't help here because:\n",
    "\n",
    "Te tells you where IMU sits on the robot (static offset)\n",
    "It doesn't tell you how the robot rotated between t and t+Δt\n",
    "That rotation is what we're COMPUTING with preintegration\n",
    "\n",
    "So You're Right AND Wrong\n",
    "\n",
    "✅ Right: We have extrinsic transforms\n",
    "✅ Right: We'll use those later in the full calibration\n",
    "❌ Wrong: They don't replace the need to rotate during preintegration\n",
    "\n",
    "The extrinsic Te is used AFTER preintegration to transform between sensor coordinate frames. The preintegration computes motion in the body frame first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
